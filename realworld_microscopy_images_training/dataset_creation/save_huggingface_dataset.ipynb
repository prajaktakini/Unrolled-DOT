{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T21:32:10.889184Z",
     "start_time": "2025-05-22T21:32:03.342720Z"
    }
   },
   "source": "!pip install datasets",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\r\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from datasets) (3.18.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from datasets) (2.2.6)\r\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\r\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\r\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting pandas (from datasets)\r\n",
      "  Downloading pandas-2.2.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (89 kB)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from datasets) (2.32.3)\r\n",
      "Collecting tqdm>=4.66.3 (from datasets)\r\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting xxhash (from datasets)\r\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting multiprocess<0.70.17 (from datasets)\r\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\r\n",
      "  Downloading huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from datasets) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from datasets) (6.0.2)\r\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\r\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (16 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading multidict-6.4.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.3 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading propcache-0.3.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (10 kB)\r\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\r\n",
      "  Downloading yarl-1.20.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (72 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.13.2)\r\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\r\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\r\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/unrolled-dot-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\r\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\r\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\r\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\r\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-macosx_11_0_arm64.whl (457 kB)\r\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\r\n",
      "Downloading multidict-6.4.4-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\r\n",
      "Downloading yarl-1.20.0-cp310-cp310-macosx_11_0_arm64.whl (94 kB)\r\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\r\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-macosx_11_0_arm64.whl (122 kB)\r\n",
      "Downloading huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\r\n",
      "Downloading propcache-0.3.1-cp310-cp310-macosx_11_0_arm64.whl (45 kB)\r\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-macosx_12_0_arm64.whl (30.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.8/30.8 MB\u001B[0m \u001B[31m47.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Downloading pandas-2.2.3-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.3/11.3 MB\u001B[0m \u001B[31m53.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\r\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\r\n",
      "Downloading xxhash-3.5.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\r\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\r\n",
      "\u001B[2K  Attempting uninstall: fsspec\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 4/19\u001B[0m [pyarrow]\r\n",
      "\u001B[2K    Found existing installation: fsspec 2025.3.2━━━━━━━━━━━━━━\u001B[0m \u001B[32m 4/19\u001B[0m [pyarrow]\r\n",
      "\u001B[2K    Uninstalling fsspec-2025.3.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 4/19\u001B[0m [pyarrow]\r\n",
      "\u001B[2K      Successfully uninstalled fsspec-2025.3.2━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m 4/19\u001B[0m [pyarrow]\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19/19\u001B[0m [datasets]/19\u001B[0m [datasets]ce-hub]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 fsspec-2025.3.0 huggingface-hub-0.31.4 multidict-6.4.4 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 tqdm-4.67.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T21:38:31.082366Z",
     "start_time": "2025-05-22T21:38:31.079199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Value, Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your folders\n",
    "ground_truth_dir = \"/Users/prajakta/Downloads/psf5_gt_cropped\"\n",
    "psf_convolved_dir =  \"/Users/prajakta/Downloads/psf5_dat_cropped\""
   ],
   "id": "f10feaa9626529fe",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T21:38:32.814786Z",
     "start_time": "2025-05-22T21:38:32.099026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Match files based on common prefix\n",
    "gt_files = sorted([f for f in os.listdir(ground_truth_dir) if f.endswith(\"_gt.png\")])\n",
    "psf_files = sorted([f for f in os.listdir(psf_convolved_dir) if f.endswith(\"_convolved.png\")])\n",
    "\n",
    "# Extract common base names (e.g., '100_B11_1_blue_maxcrop')\n",
    "gt_basenames = {f.replace(\"_gt.png\", \"\") for f in gt_files}\n",
    "psf_basenames = {f.replace(\"_convolved.png\", \"\") for f in psf_files}\n",
    "\n",
    "common_basenames = sorted(gt_basenames & psf_basenames)\n",
    "\n",
    "# Step 2: Create file pairs\n",
    "file_pairs = [{\n",
    "    \"image\": os.path.join(psf_convolved_dir, f\"{basename}_convolved.png\"),\n",
    "    \"label\": os.path.join(ground_truth_dir, f\"{basename}_gt.png\")\n",
    "} for basename in common_basenames]\n",
    "\n",
    "print(f\"✅ Matched {len(file_pairs)} image pairs.\")\n",
    "\n",
    "# Step 3: Convert to HF Dataset\n",
    "df = pd.DataFrame(file_pairs)\n",
    "\n",
    "features = Features({\n",
    "    \"image\": Image(),\n",
    "    \"label\": Image()\n",
    "})\n",
    "\n",
    "dataset = Dataset.from_pandas(df, features=features)\n",
    "\n",
    "# Step 4: Save locally (optional before uploading)\n",
    "output_path = \"/Users/prajakta/Downloads/human_protein_atlas_cells_dataset\"\n",
    "dataset.save_to_disk(output_path)\n",
    "\n",
    "print(f\"📁 Dataset saved to {output_path}\")"
   ],
   "id": "82b6c39bdd197f67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched 1000 image pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 1497.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Dataset saved to /Users/prajakta/Downloads/human_protein_atlas_cells_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T22:19:45.974831Z",
     "start_time": "2025-05-22T22:19:45.609523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"/Users/prajakta/Downloads/human_protein_atlas_cells_dataset\")\n",
    "\n",
    "print(dataset[0].keys())  # ✅ ['image', 'label']\n",
    "dataset[0]['image'].show()\n",
    "dataset[0]['label'].show()"
   ],
   "id": "15a16562c95dd1d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'label'])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T22:23:36.856853Z",
     "start_time": "2025-05-22T22:22:45.507135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_from_disk\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Use this in terminal or script (not in Jupyter)\n",
    "login(token=\"TEST_SECRET\")\n",
    "\n",
    "# Load your dataset from local disk\n",
    "dataset = load_from_disk(\"/Users/prajakta/Downloads/human_protein_atlas_cells_dataset\")\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "dataset.push_to_hub(\"prajaktakini/human_protein_atlas_cells_dataset\")"
   ],
   "id": "c0e2454356ba4329",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\u001B[A\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 6618.62 examples/s][A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]\u001B[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 86.52ba/s][A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:49<00:00, 49.65s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/prajaktakini/human_protein_atlas_cells_dataset/commit/34c33a895efd6d1a589529a718316a1b91301ae5', commit_message='Upload dataset', commit_description='', oid='34c33a895efd6d1a589529a718316a1b91301ae5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/prajaktakini/human_protein_atlas_cells_dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='prajaktakini/human_protein_atlas_cells_dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T00:58:14.327730Z",
     "start_time": "2025-05-24T00:58:03.845810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "from PIL import Image  # Add this import\n",
    "resize_shape = (128, 128)  # Desired image size\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "hf_dataset = load_dataset(\"prajaktakini/human_protein_atlas_cells_dataset\", split=\"train\")\n",
    "\n",
    "print(hf_dataset.shape)\n",
    "print(hf_dataset[0])\n",
    "\n",
    "# Inspect PSF value range to validate normalization\n",
    "\n",
    "sample_vals = [np.array(sample[\"image\"].convert(\"L\"), dtype=np.float32) for sample in hf_dataset.select(range(1000))]\n",
    "psf_max = np.max([np.max(im) for im in sample_vals])\n",
    "psf_mean = np.mean([np.mean(im) for im in sample_vals])\n",
    "print(f\"🔍 PSF Sample Max: {psf_max:.2f}, Mean: {psf_mean:.2f}\")\n",
    "if psf_max < 10:\n",
    "    print(\"✅ measNormalization ~5 is likely appropriate.\")\n",
    "else:\n",
    "    print(\"⚠️ Consider adjusting measNormalization. Peak PSF intensity is high.\")\n",
    "\n",
    "# Set normalization\n",
    "measNormalization = 77.77\n",
    "\n",
    "# Accumulate images\n",
    "diff_L = []     # PSF (input)\n",
    "truthIms = []   # Ground truth\n",
    "\n",
    "for sample in hf_dataset:\n",
    "    psf_img = sample[\"image\"].convert(\"L\").resize(resize_shape, Image.BICUBIC)\n",
    "    gt_img = sample[\"label\"].convert(\"L\").resize(resize_shape, Image.BICUBIC)\n",
    "\n",
    "    psf_np = np.array(psf_img, dtype=np.float32) / measNormalization\n",
    "    gt_np = np.array(gt_img, dtype=np.float32)\n",
    "\n",
    "    diff_L.append(psf_np)\n",
    "    truthIms.append(gt_np)\n",
    "\n",
    "# Stack to [H, W, N]\n",
    "diff_L = np.stack(diff_L, axis=2)\n",
    "truthIms = np.stack(truthIms, axis=2)\n",
    "\n",
    "# Save to .mat\n",
    "output_matfile = \"hf_microscopy_dataset.mat\"\n",
    "savemat(output_matfile, {\n",
    "    \"diff_L\": diff_L,\n",
    "    \"truthIms\": truthIms\n",
    "})\n",
    "\n",
    "print(f\"✅ Saved {output_matfile} with shape diff_L: {diff_L.shape}, truthIms: {truthIms.shape}\")"
   ],
   "id": "34b8e6d0fc0dd244",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 1200.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x512 at 0x11E53F9D0>, 'label': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x512 at 0x11E53FB80>}\n",
      "🔍 PSF Sample Max: 255.00, Mean: 77.77\n",
      "⚠️ Consider adjusting measNormalization. Peak PSF intensity is high.\n",
      "✅ Saved hf_microscopy_dataset.mat with shape diff_L: (128, 128, 1000), truthIms: (128, 128, 1000)\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
